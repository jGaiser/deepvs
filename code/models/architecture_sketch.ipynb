{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch_geometric\n",
    "import random\n",
    "import yaml\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import remove_isolated_nodes\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCN2Conv\n",
    "from torch_geometric.nn import SAGPooling\n",
    "from torch_geometric.nn import MLP\n",
    "from torch_geometric.nn import AttentiveFP\n",
    "from torch_geometric.nn.aggr import AttentionalAggregation\n",
    "from copy import deepcopy \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('/xdisk/twheeler/jgaiser/deepvs/deepvs/data/protein_config.yaml', 'r') as config_file:\n",
    "    protein_config = yaml.safe_load(config_file) \n",
    "\n",
    "PROTEIN_ATOM_LABELS = protein_config['atom_labels']\n",
    "PROTEIN_EDGE_LABELS = protein_config['edge_labels']\n",
    "INTERACTION_LABELS = protein_config['interaction_labels']\n",
    "PDB_IDS = []\n",
    "\n",
    "dummy_index = PROTEIN_ATOM_LABELS.index('DUMMY')\n",
    "voxel_edge_index = PROTEIN_EDGE_LABELS.index('voxel')\n",
    "\n",
    "pocket_graph_dir = '/xdisk/twheeler/jgaiser/deepvs/deepvs/data/graph_data/pockets/'\n",
    "mol_graph_dir = '/xdisk/twheeler/jgaiser/deepvs/deepvs/data/graph_data/molecules/'\n",
    "\n",
    "pocket_file_structure = pocket_graph_dir + \"%s_pocket_graph.pkl\"\n",
    "mol_file_structure = mol_graph_dir + \"%s_mol_graph.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_IDS = []\n",
    "\n",
    "for item in glob.glob(pocket_graph_dir + \"*\"):\n",
    "    PDB_IDS.append(item.split('/')[-1].split('_')[0])\n",
    "\n",
    "PDB_IDS = np.array(PDB_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_training_batch(pdb_ids, batch_size):\n",
    "    sample_ids = np.random.choice(PDB_IDS, batch_size)\n",
    "    pocket_graphs = [] \n",
    "    mol_graphs = []\n",
    "    decoy_graphs = []\n",
    "    \n",
    "    for s_id in sample_ids:\n",
    "        pocket_graphs.append(pickle.load(open(pocket_file_structure % s_id, 'rb')))\n",
    "        mol_graphs.append(pickle.load(open(mol_file_structure % s_id, 'rb')))\n",
    "        decoy_graphs.append(pickle.load(open(mol_file_structure % np.random.choice(PDB_IDS), 'rb')))\n",
    "    \n",
    "    mol_graphs += decoy_graphs\n",
    "    \n",
    "    pocket_loader = DataLoader(pocket_graphs, batch_size=batch_size, shuffle=False)\n",
    "    mol_loader = DataLoader(mol_graphs, batch_size=batch_size*2, shuffle=False)\n",
    "    \n",
    "    return next(iter(pocket_loader)), next(iter(mol_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['halogenbond', 'hbond_a', 'hbond_d', 'hydroph_interaction', 'pication_c', 'pication_r', 'pistack', 'saltbridge_n', 'saltbridge_p']\n",
      "[1299, 40523, 30777, 63876, 547, 7661, 36457, 6324, 2158]\n"
     ]
    }
   ],
   "source": [
    "interaction_count = [0 for x in INTERACTION_LABELS]\n",
    "\n",
    "for s_id in PDB_IDS:\n",
    "    g = (pickle.load(open(mol_file_structure % s_id, 'rb')))\n",
    "    interacting_voxel_indices = torch.where(torch.sum(g.y, dim=1) > 0)[0]\n",
    "    for idx in torch.where(g.y[interacting_voxel_indices] > 0)[1]:\n",
    "        interaction_count[idx.item()] += 1\n",
    "        \n",
    "print(INTERACTION_LABELS)\n",
    "print(interaction_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49.17321016166282, 1.5762900081435234, 2.075445949897651, 1.0, 116.77513711151735, 8.337814906670147, 1.7520915050607564, 10.10056925996205, 29.599629286376274]\n"
     ]
    }
   ],
   "source": [
    "interaction_weights = [1/(x/max(interaction_count)) for x in interaction_count]\n",
    "print(interaction_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Event:\n",
    "#     def __init__(self, sr1=None, foobar=None):\n",
    "#         self.sr1 = sr1\n",
    "#         self.foobar = foobar\n",
    "#         self.state = STATE_NON_EVENT\n",
    " \n",
    "# # Event class wrappers to provide syntatic sugar\n",
    "# class TypeTwoEvent(Event):\n",
    "#     def __init__(self, level=None):\n",
    "#         self.sr1 = level\n",
    "#         self.state = STATE_EVENT_TWO\n",
    "        \n",
    "# class TypeTwoEvent(Event):\n",
    "#     def __init__(self, level=None, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.sr1 = level\n",
    "#         self.state = STATE_EVENT_TWO\n",
    "from torch_geometric.nn import GATConv, MessagePassing, global_add_pool\n",
    "\n",
    "class AtomicAttentiveFP(AttentiveFP):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.voxel_classifier = nn.Linear(kwargs['hidden_channels'], 9)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        \"\"\"\"\"\"\n",
    "        # Atom Embedding:\n",
    "        x = F.leaky_relu_(self.lin1(x))\n",
    "\n",
    "        h = F.elu_(self.atom_convs[0](x, edge_index, edge_attr))\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        x = self.atom_grus[0](h, x).relu_()\n",
    "\n",
    "        for conv, gru in zip(self.atom_convs[1:], self.atom_grus[1:]):\n",
    "            h = F.elu_(conv(x, edge_index))\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            x = gru(h, x).relu_()\n",
    "\n",
    "        # Molecule Embedding:\n",
    "        row = torch.arange(batch.size(0), device=batch.device)\n",
    "        edge_index = torch.stack([row, batch], dim=0)\n",
    "\n",
    "        out = global_add_pool(x, batch).relu_()\n",
    "        \n",
    "        for t in range(self.num_timesteps):\n",
    "            h = F.elu_(self.mol_conv((x, out), edge_index))\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            out = self.mol_gru(h, out).relu_()\n",
    "\n",
    "        # Predictor:\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "        return self.voxel_classifier(x), self.lin2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def forward(self, x, edge_index, edge_attr, batch):\n",
      "        \"\"\"\"\"\"\n",
      "        # Atom Embedding:\n",
      "        x = F.leaky_relu_(self.lin1(x))\n",
      "\n",
      "        h = F.elu_(self.atom_convs[0](x, edge_index, edge_attr))\n",
      "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
      "        x = self.atom_grus[0](h, x).relu_()\n",
      "\n",
      "        for conv, gru in zip(self.atom_convs[1:], self.atom_grus[1:]):\n",
      "            h = F.elu_(conv(x, edge_index))\n",
      "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
      "            x = gru(h, x).relu_()\n",
      "\n",
      "        # Molecule Embedding:\n",
      "        row = torch.arange(batch.size(0), device=batch.device)\n",
      "        edge_index = torch.stack([row, batch], dim=0)\n",
      "\n",
      "        out = global_add_pool(x, batch).relu_()\n",
      "        for t in range(self.num_timesteps):\n",
      "            h = F.elu_(self.mol_conv((x, out), edge_index))\n",
      "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
      "            out = self.mol_gru(h, out).relu_()\n",
      "\n",
      "        # Predictor:\n",
      "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
      "        return self.lin2(out)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "print(\"\".join(inspect.getsourcelines(AttentiveFP.forward)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6906, dtype=torch.float64,\n",
       "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## MOLECULE GRAPH POOLING \n",
    "#@title Molecule Pooling\n",
    "\n",
    "mol_hidden = 512 \n",
    "\n",
    "class MoleculePool(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = AtomicAttentiveFP(in_channels=52, \n",
    "                                 hidden_channels=mol_hidden, \n",
    "                                 out_channels=mol_hidden,\n",
    "                                 edge_dim=10,\n",
    "                                 num_layers=5,\n",
    "                                 num_timesteps=5,\n",
    "                                 dropout=0.0)\n",
    "    \n",
    "        \n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weights, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        h = self.conv1(x, edge_index, edge_weights, batch)\n",
    "        return h\n",
    "    \n",
    "model = MoleculePool()\n",
    "pocket_batch, mol_batch = fetch_training_batch(PDB_IDS, 32)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "atom_embeds, mol_embed = model(mol_batch)\n",
    "interacting_voxel_indices = torch.where(torch.sum(mol_batch.y, dim=1) > 0)[0]\n",
    "criterion(atom_embeds[interacting_voxel_indices], mol_batch.y[interacting_voxel_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "sigmoid = nn.Sigmoid()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(interaction_weights)).to(device)\n",
    "model = MoleculePool().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def mol_training_loop(epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"EPOCH %s\" % epoch)\n",
    "        \n",
    "        for batch_idx in range(int(len(PDB_IDS) / BATCH_SIZE)):\n",
    "            loss_history = []\n",
    "            pocket_batch, mol_batch = fetch_training_batch(PDB_IDS, 32)\n",
    "            mol_batch = mol_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            atom_embeds, mol_embed = model(mol_batch)\n",
    "\n",
    "            interacting_voxel_indices = torch.where(torch.sum(mol_batch.y, dim=1) > 0)[0]\n",
    "            loss = criterion(atom_embeds[interacting_voxel_indices], \n",
    "                             mol_batch.y[interacting_voxel_indices].to(device))\n",
    "                               \n",
    "            \n",
    "            loss_history.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(sum(loss_history) / len(loss_history))\n",
    "                loss_history = []\n",
    "\n",
    "# mol_training_loop(100, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6919, dtype=torch.float64,\n",
       "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## POCKET GRAPH GCN\n",
    "pocket_hidden = 512 \n",
    "NODE_DIMS = 41 \n",
    "EDGE_DIMS = 9 \n",
    "DUMMY_INDEX = protein_config['atom_labels'].index('DUMMY')\n",
    "\n",
    "edge_weight_modifier = 1\n",
    "MAX_EDGE_WEIGHT = 15.286330223083496 / edge_weight_modifier\n",
    "\n",
    "class PocketGCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(NODE_DIMS-3,pocket_hidden)\n",
    "        \n",
    "        self.conv1 = GCN2Conv(pocket_hidden, 0.2)\n",
    "        self.conv2 = GCN2Conv(pocket_hidden, 0.2)\n",
    "        self.conv3 = GCN2Conv(pocket_hidden, 0.2)\n",
    "        \n",
    "        self.voxel_prediction = nn.Linear(pocket_hidden,9)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weights = data.x[:,:-3], data.edge_index, data.edge_attr[:,-1] / MAX_EDGE_WEIGHT \n",
    "\n",
    "        x = self.linear1(x)\n",
    "    \n",
    "        h = self.conv1(x, x, edge_index, edge_weights)\n",
    "        h = F.relu(h)\n",
    "\n",
    "        h = self.conv2(h, x, edge_index, edge_weights)\n",
    "        h = F.relu(h)\n",
    "\n",
    "        h = self.conv3(h, x, edge_index, edge_weights)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        return self.voxel_prediction(h), h\n",
    "        \n",
    "        \n",
    "model = PocketGCN()\n",
    "pocket_batch, mol_batch = fetch_training_batch(PDB_IDS, 16)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "voxel_embeds, _ = model(pocket_batch)\n",
    "interacting_voxel_indices = torch.where(torch.sum(pocket_batch.y, dim=1) > 0)[0]\n",
    "criterion(voxel_embeds[interacting_voxel_indices], pocket_batch.y[interacting_voxel_indices])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "0.8772057591976178\n",
      "0.8814792667468496\n",
      "0.7724420435185684\n",
      "0.6317582452860929\n",
      "0.7521522869624137\n",
      "0.6236169924374215\n",
      "0.6265521881615626\n",
      "0.7595097036211796\n",
      "0.7058004424531212\n",
      "0.795169066033177\n",
      "0.8426764223555572\n",
      "0.8912911965793044\n",
      "0.7254604871552458\n",
      "0.5961641725174945\n",
      "0.678087353566586\n",
      "0.788790090883841\n",
      "0.7639377332313084\n",
      "0.7552595334430137\n",
      "0.6195694806414231\n",
      "0.5415449231132282\n",
      "0.5644196772990164\n",
      "0.8102796001289015\n",
      "0.640316067583322\n",
      "0.6115450794109032\n",
      "0.5648700620592927\n",
      "0.6466832170772769\n",
      "0.533578156996897\n",
      "0.5144788142505027\n",
      "0.5524863108999009\n",
      "0.5924881660431535\n",
      "0.4917460778547627\n",
      "0.6086022815085812\n",
      "0.5535892681379121\n",
      "0.5891871367752499\n",
      "0.7498581800686909\n",
      "0.5035398636149109\n",
      "0.8458210700549625\n",
      "0.574805321329095\n",
      "0.4789835999375726\n",
      "0.6299925963519087\n",
      "0.5713546522633901\n",
      "0.4814444000888872\n",
      "0.531038688582531\n",
      "0.5440264997758042\n",
      "0.6602316471958343\n",
      "0.578405628090927\n",
      "0.5320423853722986\n",
      "0.4563847244164438\n",
      "0.4450537128741511\n",
      "0.4277876474183357\n",
      "0.6816472518592002\n",
      "0.5447116970352348\n",
      "0.7025937149777076\n",
      "0.4932850025450316\n",
      "0.6027353036744196\n",
      "0.5079784003973604\n",
      "0.5496636512953874\n",
      "0.5816300754669834\n",
      "0.458664521723766\n",
      "0.4247864215333673\n",
      "0.490041612585378\n",
      "0.5413871545730948\n",
      "0.5313808472269366\n",
      "0.488156558307055\n",
      "0.4962254241067076\n",
      "0.6228082934244163\n",
      "0.5785384316618043\n",
      "0.6734297578837889\n",
      "0.8979330297073143\n",
      "0.6058336172928467\n",
      "0.604498063701229\n",
      "0.3826245787491383\n",
      "0.5145900877631554\n",
      "0.536957158330138\n",
      "0.3786644571794038\n",
      "0.49141838798552956\n",
      "0.6109675458378082\n",
      "0.6685628605996762\n",
      "0.43154948228447454\n",
      "0.5458526732299388\n",
      "0.43323183191760883\n",
      "0.672074007925159\n",
      "0.4829992061805012\n",
      "0.5012048586595372\n",
      "0.5505376985658573\n",
      "0.49574046122040416\n",
      "0.4933087764834748\n",
      "0.43813321060076277\n",
      "0.46553123790571094\n",
      "0.5801471993245995\n",
      "0.4840661006984687\n",
      "0.46986493318669537\n",
      "0.687530498018959\n",
      "0.4173020510059614\n",
      "0.6047718253809552\n",
      "0.5264293096988021\n",
      "0.4671091223475595\n",
      "0.46541107965776973\n",
      "0.5167099741961972\n",
      "0.48683643053170567\n",
      "0.48553200923765927\n",
      "0.49830856871345536\n",
      "0.5299071001893182\n",
      "0.4398265462550812\n",
      "0.4822613279810067\n",
      "0.48660970804893433\n",
      "0.4060739475066149\n",
      "0.5155949771307049\n",
      "0.3737862260100028\n",
      "0.5072175667110224\n",
      "0.40760114613115606\n",
      "0.48367224460450964\n",
      "0.4375341219638585\n",
      "0.7057771012211934\n",
      "0.5363159818072178\n",
      "0.6281744271685884\n",
      "0.5580802734523475\n",
      "0.5261480094898456\n",
      "0.4739029064459196\n",
      "0.6133380907662386\n",
      "0.46036926021235175\n",
      "0.4350036712413819\n",
      "EPOCH 1\n",
      "0.47485690456294105\n",
      "0.4767801123936533\n",
      "0.4336659414274862\n",
      "0.42909170099555427\n",
      "0.4271519234081142\n",
      "0.5544884439095731\n",
      "0.4533163044396715\n",
      "0.6582741737538742\n",
      "0.4420216656882879\n",
      "0.4539496464229352\n",
      "0.5104986645857245\n",
      "0.4765058184404645\n",
      "0.37916044815641414\n",
      "0.49009681025542706\n",
      "0.6240602619819738\n",
      "0.5776126165000258\n",
      "0.43380739566916837\n",
      "0.4082964432907853\n",
      "0.48054928136370834\n",
      "0.4810192043919749\n",
      "0.4932540702777799\n",
      "0.4229219084056664\n",
      "0.44190712948468647\n",
      "0.7076231452692765\n",
      "0.43942031060615727\n",
      "0.5282991521934512\n",
      "0.5475789374854788\n",
      "0.4685985311771091\n",
      "0.4804669580502281\n",
      "0.42320859016863244\n",
      "0.5173579763701672\n",
      "0.49787566339358125\n",
      "0.4783629993113252\n",
      "0.4493848642108507\n",
      "0.529370202821476\n",
      "0.4140483388685877\n",
      "0.42618248353516464\n",
      "0.5735397439416542\n",
      "0.47398361319518995\n",
      "0.47774031034709763\n",
      "0.4050357429260111\n",
      "0.5669908817033109\n",
      "0.4526677604826514\n",
      "0.37624007275325416\n",
      "0.4907839702389425\n",
      "0.45585584079624275\n",
      "0.6121899311410886\n",
      "0.7820223162445658\n",
      "0.40565773725532045\n",
      "0.4035518140605897\n",
      "0.4420624650000586\n",
      "0.46175755826194026\n",
      "0.491522609534648\n",
      "0.4534535359357008\n",
      "0.43981876046292556\n",
      "0.4534194276993172\n",
      "0.46902021977279645\n",
      "0.460799184809137\n",
      "0.45640233266564284\n",
      "0.49735628253992575\n",
      "0.48679601730238803\n",
      "0.45772105578938477\n",
      "0.47731452398348484\n",
      "0.432481673373198\n",
      "0.3999061808696374\n",
      "0.5081152068381904\n",
      "0.4729620229171183\n",
      "0.4675939103256136\n",
      "0.535677184640332\n",
      "0.38504096254877634\n",
      "0.638504634081977\n",
      "0.49323281519555695\n",
      "0.5909333516299254\n",
      "0.4899805218779861\n",
      "0.6608822919198473\n",
      "0.6383176163226991\n",
      "0.4621701339750531\n",
      "0.8248200970024593\n",
      "0.3882596800188374\n",
      "0.4353434655742551\n",
      "0.4056008504424134\n",
      "0.5367435847381666\n",
      "0.47778120824081416\n",
      "0.44721681354097964\n",
      "0.4948494396307492\n",
      "0.6772621729807048\n",
      "0.5601397666973837\n",
      "0.4746544236593663\n",
      "0.599277365715335\n",
      "0.4567736527373979\n",
      "0.43780760933973023\n",
      "0.4915812226802332\n",
      "0.4855405448172614\n",
      "0.42283174965332254\n",
      "0.4354681023753838\n",
      "0.5051077532478281\n",
      "0.5461011888993436\n",
      "0.49117165242398814\n",
      "0.5672544938156416\n",
      "0.4660595693113676\n",
      "0.4459819677484705\n",
      "0.4468668829101244\n",
      "0.47291644244959696\n",
      "0.49858228497265444\n",
      "0.40614292710516625\n",
      "0.5037990216409901\n",
      "0.6051543397490238\n",
      "0.40652639707540983\n",
      "0.6964308078149318\n",
      "0.38345088620515194\n",
      "0.6457660030748897\n",
      "0.49166289303310184\n",
      "0.4122990578842369\n",
      "0.47024866667602544\n",
      "0.4377198514070855\n",
      "0.45909293049411803\n",
      "0.5920845913378809\n",
      "0.460713459856647\n",
      "0.4186176440296488\n",
      "0.45485099650815136\n",
      "0.4721019950898722\n",
      "0.44435883330872994\n",
      "EPOCH 2\n",
      "0.5700168251792005\n",
      "0.4729295506298792\n",
      "0.4150913842748484\n",
      "0.5048995143420377\n",
      "0.452892151230059\n",
      "0.4854473038968586\n",
      "0.5415989781734232\n",
      "0.4409755959005737\n",
      "0.47894259696997715\n",
      "0.4474694122887931\n",
      "0.5118115017367648\n",
      "0.41024921717416424\n",
      "0.5126609490958917\n",
      "0.5141612951410295\n",
      "0.6565076488399937\n",
      "0.5634194255087905\n",
      "0.3646789707696046\n",
      "0.4272069213991739\n",
      "0.7091839453056193\n",
      "0.5213523804389446\n",
      "0.46966127044174205\n",
      "0.4634369898871242\n",
      "0.4240427197264552\n",
      "0.49594737634435226\n",
      "0.40727011031563665\n",
      "0.5302510801759345\n",
      "0.5164697762860088\n",
      "0.468584251328875\n",
      "0.46095430965209566\n",
      "0.477832214114331\n",
      "0.46726531255786263\n",
      "0.44215433555429096\n",
      "0.4247070081215205\n",
      "0.4042470107684655\n",
      "0.5458629823688085\n",
      "0.4261530664419749\n",
      "0.5046770803395391\n",
      "0.5970679390228575\n",
      "0.48453133661730674\n",
      "0.5466463425957706\n",
      "0.43943453932088955\n",
      "0.4858857448214099\n",
      "0.5362260071368321\n",
      "0.4711650643463949\n",
      "0.4808576860655452\n",
      "0.38306900936190524\n",
      "0.46532809329402636\n",
      "0.4904006089008559\n",
      "0.48915817800173506\n",
      "0.5131558715357647\n",
      "0.4394603566036494\n",
      "0.5243010078697511\n",
      "0.43903918143848686\n",
      "0.7602852802184327\n",
      "0.5172679171106073\n",
      "0.4922534469825592\n",
      "0.444400604451127\n",
      "0.39522688695478336\n",
      "0.42808947531397223\n",
      "0.35654456222431713\n",
      "0.4478889735354256\n",
      "0.40078039133039045\n",
      "0.37831948188680947\n",
      "0.46200438967026697\n",
      "0.49873782626817037\n",
      "0.41263757566793335\n",
      "0.5150523515810456\n",
      "0.3489544574281246\n",
      "0.5838662780063124\n",
      "0.60959405755499\n",
      "0.42866187041199044\n",
      "0.5381229255310642\n",
      "0.47336197688561893\n",
      "0.4822148549250712\n",
      "0.397131167189196\n",
      "0.4623806574812984\n",
      "0.5285488087326112\n",
      "0.42211674918834957\n",
      "0.4805748254198359\n",
      "0.44021672550961743\n",
      "0.4054102554414413\n",
      "0.4742893918322042\n",
      "0.60684497324601\n",
      "0.7771066803283454\n",
      "0.47332572032115955\n",
      "0.45104980778918885\n",
      "0.47645865754306044\n",
      "0.46977980179153817\n",
      "0.5450425248349785\n",
      "0.48349620579074953\n",
      "0.46099784870685073\n",
      "0.419829028978667\n",
      "0.5528368937455257\n",
      "0.5051173658254606\n",
      "0.6726138592870085\n",
      "0.5598291544521463\n",
      "0.4506057501986442\n",
      "0.4417728248815235\n",
      "0.5759435857703187\n",
      "0.5426725619767808\n",
      "0.560881894567907\n",
      "0.4353834211288337\n",
      "0.47072033042754036\n",
      "0.36822642155949953\n",
      "0.4852071040114278\n",
      "0.5228556192638022\n",
      "0.5612413235672872\n",
      "0.3817844669638741\n",
      "0.5361524026688804\n",
      "0.47619137770727654\n",
      "0.3547737889982372\n",
      "0.44223787577801643\n",
      "0.4252420114207558\n",
      "0.5270368677363862\n",
      "0.38649195646428636\n",
      "0.612693886426801\n",
      "0.5151618819829996\n",
      "0.4889705150639266\n",
      "0.47649440144543836\n",
      "0.4869547863686694\n",
      "0.4523082938806626\n",
      "0.4448806730228622\n",
      "EPOCH 3\n",
      "0.39449889597378984\n",
      "0.49369556373572354\n",
      "0.48206314032842545\n",
      "0.4832393414645231\n",
      "0.48883194643390404\n",
      "0.45287391355514256\n",
      "0.3893353642855859\n",
      "0.38160194550695015\n",
      "0.5143208963932819\n",
      "0.4377363239204823\n",
      "0.49012810970786436\n",
      "0.4685725970141991\n",
      "0.46666863075001797\n",
      "0.48209308906760984\n",
      "0.44709512823606273\n",
      "0.4089192004579395\n",
      "0.82323168199864\n",
      "0.551438091089099\n",
      "0.3608342540731126\n",
      "0.3797001312075229\n",
      "0.3840818881502538\n",
      "0.5412674910441497\n",
      "0.4307072762616292\n",
      "0.40395657038171756\n",
      "0.372465628354158\n",
      "0.46120597399800584\n",
      "0.5995526621944343\n",
      "0.4251500869669893\n",
      "0.384629834540328\n",
      "0.4227449280956919\n",
      "0.44366611441200327\n",
      "0.4511679591610422\n",
      "0.4359342994355363\n",
      "0.41813286444103753\n",
      "0.43484974573703794\n",
      "0.43517284212901786\n",
      "0.3960080060863653\n",
      "0.4914265992496432\n",
      "0.5015139614293159\n",
      "0.47835090844301054\n",
      "0.5641957047291076\n",
      "0.35345015244711675\n",
      "0.4894232678226378\n",
      "0.534604779147266\n",
      "0.48341296296528213\n",
      "0.5533436534412589\n",
      "0.4090631983418488\n",
      "0.4126585888280697\n",
      "0.41556626564406524\n",
      "0.39248258646276324\n",
      "0.45103840227229436\n",
      "0.36891911756628015\n",
      "0.35279422943159167\n",
      "0.4496589426535605\n",
      "0.5483260922201788\n",
      "0.5540837149625509\n",
      "0.557625704918639\n",
      "0.48665082993744485\n",
      "0.4024147868245349\n",
      "0.3888614359308225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6707385126025881\n",
      "0.3757787735083245\n",
      "0.4218771783152082\n",
      "0.4626161040309235\n",
      "0.42271105843723905\n",
      "0.45566118770347713\n",
      "0.46555382887056684\n",
      "0.4056316197766852\n",
      "0.36266037034760257\n",
      "0.3786612857483997\n",
      "0.4395643615847561\n",
      "0.3852659434125195\n",
      "0.4391682265020668\n",
      "0.42190751510786356\n",
      "0.3982172193921508\n",
      "0.4814595012451262\n",
      "0.43877914238025634\n",
      "0.33852612639957524\n",
      "0.5549551048063579\n",
      "0.40834342061553414\n",
      "0.427680108906651\n",
      "0.41569122175665213\n",
      "0.4735632637067555\n",
      "0.42376589563415584\n",
      "0.4756962479475812\n",
      "0.3951722191363597\n",
      "0.43462292624832405\n",
      "0.3985626261896211\n",
      "0.43582411891571626\n",
      "0.4098631232092347\n",
      "0.3703038752973044\n",
      "0.6346856518331364\n",
      "0.48690468371163814\n",
      "0.48513766187537277\n",
      "0.3752594989726427\n",
      "0.41343833770658256\n",
      "0.4737911909382178\n",
      "0.5774099741234883\n",
      "0.40823907332113935\n",
      "0.44669661287582546\n",
      "0.37745653406699337\n",
      "0.4252433818927323\n",
      "0.44612697248639477\n",
      "0.680957339219957\n",
      "0.47204126601068624\n",
      "0.3356392367901982\n",
      "0.4487649942718764\n",
      "0.3531289759144088\n",
      "0.4332190813907578\n",
      "0.3643096709336652\n",
      "0.35986679789688286\n",
      "0.37927707199957\n",
      "0.46787462078199965\n",
      "0.43731550221602417\n",
      "0.3374721503497958\n",
      "0.4321920534883996\n",
      "0.3886870415648864\n",
      "0.44803235010803677\n",
      "0.3906546972115747\n",
      "0.5131422933657784\n",
      "0.5199788226216535\n",
      "0.578893374919377\n",
      "EPOCH 4\n",
      "0.4104282493363948\n",
      "0.5047381648933634\n",
      "0.4338251918361701\n",
      "0.41437456858159705\n",
      "0.43751655383679133\n",
      "0.43946068835705304\n",
      "0.4303222212210243\n",
      "0.40821464458280565\n",
      "0.4460994283814852\n",
      "0.4206465083929221\n",
      "0.5520002303572352\n",
      "0.41553538254489664\n",
      "0.48274499144669847\n",
      "0.4347070650134992\n",
      "0.38397632709197893\n",
      "0.40268648531221585\n",
      "0.4747076152488627\n",
      "0.4956624665811906\n",
      "0.44941279442408694\n",
      "0.40478327586336904\n",
      "0.4349324496874809\n",
      "0.40184610072035476\n",
      "0.3580418278482792\n",
      "0.42611655441125323\n",
      "0.5328566319515552\n",
      "0.36922623133130106\n",
      "0.38099155692417497\n",
      "0.4000872185656177\n",
      "0.4847918903233631\n",
      "0.39399544676895454\n",
      "0.39838605706961566\n",
      "0.4545524453025952\n",
      "0.4221109305632678\n",
      "0.36474764483695565\n",
      "0.3732244098805546\n",
      "0.47234971512005036\n",
      "0.37475921387592476\n",
      "0.7783874566790604\n",
      "0.4446741041019537\n",
      "0.44079878158824237\n",
      "0.31942415016782966\n",
      "0.36246342728589404\n",
      "0.4481081635538472\n",
      "0.5139712184568361\n",
      "0.516921337586104\n",
      "0.46483331961251473\n",
      "0.4569251284613233\n",
      "0.39245387301407486\n",
      "0.6362164696904152\n",
      "0.4445037980616513\n",
      "0.5495821651758545\n",
      "0.39445280023757506\n",
      "0.3506985725512894\n",
      "0.5020084770549385\n",
      "0.40810264059114293\n",
      "0.4569844120800458\n",
      "0.40670013470290234\n",
      "0.37884409057283935\n",
      "0.4737722269808389\n",
      "0.4765102471572795\n",
      "0.46578756184681097\n",
      "0.3838464189866388\n",
      "0.40450384669618306\n",
      "0.3594981532263595\n",
      "0.7700417618391907\n",
      "0.541018374389641\n",
      "0.6564360699040633\n",
      "0.4647607285251034\n",
      "0.4156864964908788\n",
      "0.45995325033511314\n",
      "0.6445813641147485\n",
      "0.46453470214282505\n",
      "0.49780406098993424\n",
      "0.46634958093250156\n",
      "0.4125817338357851\n",
      "0.3774694662263856\n",
      "0.4208074336152204\n",
      "0.4788647403438877\n",
      "0.42629623730258426\n",
      "0.3855575082277531\n",
      "0.48350400613536715\n",
      "0.4380294929649561\n",
      "0.376169055500328\n",
      "0.536653151831491\n",
      "0.5596316158243313\n",
      "0.378791616024391\n",
      "0.46194550016509855\n",
      "0.3499829119378937\n",
      "0.44576127144413696\n",
      "0.400486150970965\n",
      "0.3853227966512648\n",
      "0.4760473148685679\n",
      "0.4304630891201961\n",
      "0.3690795265821844\n",
      "0.47502680415722986\n",
      "0.37021845646691426\n",
      "0.39528863082002436\n",
      "0.34365197975803313\n",
      "0.45827899748072076\n",
      "0.4374710661108886\n",
      "0.40191249573381505\n",
      "0.41537298247896604\n",
      "0.4911676988933551\n",
      "0.38279917694091536\n",
      "0.4477782478282269\n",
      "0.34842713671210473\n",
      "0.37298270121208865\n",
      "0.4229000182825256\n",
      "0.5198165839588818\n",
      "0.35393701706727765\n",
      "0.3863244783965182\n",
      "0.3731808666085649\n",
      "0.3906593172362933\n",
      "0.4529987869538815\n",
      "0.6089716412399998\n",
      "0.388287406362417\n",
      "0.4112331655736999\n",
      "0.40065111660461206\n",
      "0.5887024275155787\n",
      "0.40091719466880554\n",
      "0.4435961199372805\n",
      "0.3718705496930049\n",
      "EPOCH 5\n",
      "0.35753380825685693\n",
      "0.42117473262244787\n",
      "0.39659170972032615\n",
      "0.3965310700193368\n",
      "0.378010971021294\n",
      "0.49913907214986547\n",
      "0.39296492534258437\n",
      "0.3638575851973363\n",
      "0.5228238195337995\n",
      "0.44077835736905163\n",
      "0.5137223487419037\n",
      "0.45737478845990787\n",
      "0.5072181473135138\n",
      "0.5636247412169252\n",
      "0.39739255762517506\n",
      "0.38705316138728174\n",
      "0.37432821330784366\n",
      "0.41035113569909987\n",
      "0.5041385493946847\n",
      "0.5915301088363875\n",
      "0.41168256175787976\n",
      "0.30941435112372934\n",
      "0.3162871003570174\n",
      "0.4199084130515483\n",
      "0.3916184925070282\n",
      "0.39560034221744766\n",
      "0.36037126955171767\n",
      "0.3900868666982294\n",
      "0.40753433789611465\n",
      "0.3559314497462383\n",
      "0.4362841398032044\n",
      "0.4935521969007637\n",
      "0.42122367803554334\n",
      "0.4043573314252385\n",
      "0.3898258463549296\n",
      "0.36749983505518197\n",
      "0.5546189069883268\n",
      "0.3603223343518658\n",
      "0.6189593091709302\n",
      "0.42926381868914687\n",
      "0.33477813556152436\n",
      "0.4389710769214056\n",
      "0.3644879613491344\n",
      "0.45713591806131965\n",
      "0.3563837634518937\n",
      "0.4976866882373279\n",
      "0.3772308180217144\n",
      "0.5954529236271453\n",
      "0.4049958714410094\n",
      "0.4757532272123242\n",
      "0.541752226225097\n",
      "0.4383013353832525\n",
      "0.5154163070300848\n",
      "0.35839815213757636\n",
      "0.482869756774147\n",
      "0.6267955380723512\n",
      "0.5455161142069288\n",
      "0.4101521291982972\n",
      "0.3898745460859722\n",
      "0.48751255986701325\n",
      "0.4333706084859409\n",
      "0.4922885892086872\n",
      "0.5158788180498668\n",
      "0.3758220698983722\n",
      "0.6153326713617981\n",
      "0.4634223370607501\n",
      "0.5209524865434123\n",
      "0.6139643818419238\n",
      "0.3739467464373682\n",
      "0.5046745525433665\n",
      "0.36718131215672634\n",
      "0.382899574402734\n",
      "0.3842195926258215\n",
      "0.4432645635572014\n",
      "0.38869071906216124\n",
      "0.4054051010605458\n",
      "0.44696008874594073\n",
      "0.4152556514951583\n",
      "0.4802993239016489\n",
      "0.3760871861831541\n",
      "0.4439689996008325\n",
      "0.5256211613435159\n",
      "0.35725223335999606\n",
      "0.4519197949717743\n",
      "0.4555887404350488\n",
      "0.42783794667518993\n",
      "0.3975434944730598\n",
      "0.42023011993615866\n",
      "0.4317253592746311\n",
      "0.3656415133176942\n",
      "0.42136734916922136\n",
      "0.4198886369655753\n",
      "0.36160951111894485\n",
      "0.4415504135747254\n",
      "0.5007443024462476\n",
      "0.5093551838105469\n",
      "0.3457384343369736\n",
      "0.3775286257347809\n",
      "0.4333600070466817\n",
      "0.4808130718604587\n",
      "0.5299328819438098\n",
      "0.44974434287022663\n",
      "0.44169288046570754\n",
      "0.3498537023123018\n",
      "0.32512343804652416\n",
      "0.4222725906733898\n",
      "0.4191648276201666\n",
      "0.37848289637411753\n",
      "0.46623781192041025\n",
      "0.427352321368833\n",
      "0.3937363759297408\n",
      "0.42208244540358025\n",
      "0.39168794606247603\n",
      "0.3972921898612617\n",
      "0.3753401060897698\n",
      "0.4335998391720073\n",
      "0.4060891689413702\n",
      "0.42609145437180007\n",
      "0.3755381174007101\n",
      "0.42342540863936207\n",
      "0.3933267878641722\n",
      "0.37529680650264985\n",
      "EPOCH 6\n",
      "0.40001916392058856\n",
      "0.37485224082357266\n",
      "0.48290627113743184\n",
      "0.4212953428557155\n",
      "0.3404984747659213\n",
      "0.5302282170186733\n",
      "0.464000375646705\n",
      "0.43538369503370644\n",
      "0.45415782796472964\n",
      "0.38102970808507536\n",
      "0.3989755273076156\n",
      "0.6199610102707426\n",
      "0.4626801681201708\n",
      "0.43382239211318024\n",
      "0.43075614592594663\n",
      "0.3997022339227182\n",
      "0.3744113457226276\n",
      "0.4135872526289374\n",
      "0.3249303206983324\n",
      "0.3601236369561565\n",
      "0.47750175032979797\n",
      "0.45337693372766164\n",
      "0.5013473283989016\n",
      "0.42511502306252846\n",
      "0.48482674546822957\n",
      "0.38640521809957895\n",
      "0.3947017091506612\n",
      "0.36684092185304984\n",
      "0.3096256418019459\n",
      "0.3787511420784712\n",
      "0.43758679693832386\n",
      "0.42073883708804816\n",
      "0.42075643298370086\n",
      "0.5342697580507225\n",
      "0.35753105688681075\n",
      "0.5937509355107083\n",
      "0.612926845046944\n",
      "0.4616497552523816\n",
      "0.3920354215932295\n",
      "0.40892769874811774\n",
      "0.4318843051856747\n",
      "0.44490496160089443\n",
      "0.4503777753000884\n",
      "0.48655248185632666\n",
      "0.4039033243681921\n",
      "0.43856218901510136\n",
      "0.4773092136910327\n",
      "0.4688930808386711\n",
      "0.42864238352700096\n",
      "0.44290385079871913\n",
      "0.3813598061222821\n",
      "0.40826801489756465\n",
      "0.38554007808398993\n",
      "0.41155380215367793\n",
      "0.4715125346425823\n",
      "0.43972850485326037\n",
      "0.36030305115768235\n",
      "0.40867335197927507\n",
      "0.4366141080729509\n",
      "0.3764810163197569\n",
      "0.6022254661974081\n",
      "0.37835701029464813\n",
      "0.4392280064530924\n",
      "0.41284915306684583\n",
      "0.3813332740696839\n",
      "0.4147886267933095\n",
      "0.3697127171356269\n",
      "0.32056173056909254\n",
      "0.5561383096994201\n",
      "0.436801126295752\n",
      "0.4659026957011688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(loss_history) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loss_history))\n\u001b[1;32m     32\u001b[0m                 loss_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 34\u001b[0m \u001b[43mmol_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [18], line 14\u001b[0m, in \u001b[0;36mmol_training_loop\u001b[0;34m(epochs, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(PDB_IDS) \u001b[38;5;241m/\u001b[39m batch_size)):\n\u001b[1;32m     13\u001b[0m     loss_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 14\u001b[0m     pocket_batch, mol_batch \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_training_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPDB_IDS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     pocket_batch \u001b[38;5;241m=\u001b[39m pocket_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n",
      "Cell \u001b[0;32mIn [4], line 8\u001b[0m, in \u001b[0;36mfetch_training_batch\u001b[0;34m(pdb_ids, batch_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m decoy_graphs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s_id \u001b[38;5;129;01min\u001b[39;00m sample_ids:\n\u001b[0;32m----> 8\u001b[0m     pocket_graphs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpocket_file_structure\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m     mol_graphs\u001b[38;5;241m.\u001b[39mappend(pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(mol_file_structure \u001b[38;5;241m%\u001b[39m s_id, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m     10\u001b[0m     decoy_graphs\u001b[38;5;241m.\u001b[39mappend(pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(mol_file_structure \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(PDB_IDS), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "File \u001b[0;32m~/.conda/envs/vs-env/lib/python3.10/site-packages/torch/storage.py:221\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_cuda\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_from_bytes\u001b[39m(b):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(io\u001b[38;5;241m.\u001b[39mBytesIO(b))\n\u001b[1;32m    225\u001b[0m _StorageBase\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m=\u001b[39m _type  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "sigmoid = nn.Sigmoid()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(interaction_weights)).to(device)\n",
    "model = PocketGCN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def mol_training_loop(epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"EPOCH %s\" % epoch)\n",
    "        \n",
    "        for batch_idx in range(int(len(PDB_IDS) / batch_size)):\n",
    "            loss_history = []\n",
    "            pocket_batch, mol_batch = fetch_training_batch(PDB_IDS, 32)\n",
    "            pocket_batch = pocket_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            voxel_embeds,_ = model(pocket_batch)\n",
    "\n",
    "            interacting_voxel_indices = torch.where(torch.sum(pocket_batch.y, dim=1) > 0)[0]\n",
    "            loss = criterion(voxel_embeds[interacting_voxel_indices], \n",
    "                             pocket_batch.y[interacting_voxel_indices].to(device))\n",
    "                               \n",
    "            \n",
    "            loss_history.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(sum(loss_history) / len(loss_history))\n",
    "                loss_history = []\n",
    "\n",
    "mol_training_loop(100, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POCKET GRAPH POOLING\n",
    "pool_hidden = 512 \n",
    "\n",
    "class PoxelPool(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "#         self.conv1 = GCN2Conv(pool_hidden, 0.1)\n",
    "#         self.conv2 = GCN2Conv(pool_hidden, 0.1)\n",
    "\n",
    "        self.pool1 = SAGPooling(pool_hidden)\n",
    "\n",
    "        self.conv3 = GCN2Conv(pool_hidden, 0.2)\n",
    "        self.conv4 = GCN2Conv(pool_hidden, 0.2)\n",
    "\n",
    "        self.pool2 = SAGPooling(pool_hidden)\n",
    "\n",
    "        gate_nn = MLP([pool_hidden, 1], act='relu')\n",
    "        nn = MLP([pool_hidden, pool_hidden], act='relu')\n",
    "        self.global_pool = AttentionalAggregation(gate_nn, nn)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weights, batch = data.x, data.edge_index, data.edge_attr[:,-1], data.batch\n",
    "        \n",
    "#         h = self.conv1(x, x, edge_index, edge_weights)\n",
    "#         h = F.relu(h)\n",
    "\n",
    "#         h = self.conv2(h, x, edge_index, edge_weights)\n",
    "#         h = F.relu(h)\n",
    "\n",
    "        h, edge_index, edge_weights, batch, _, _ = self.pool1(x, edge_index, edge_weights, batch)\n",
    "\n",
    "        h = self.conv3(h, h, edge_index, edge_weights)\n",
    "        h = F.relu(h)\n",
    "\n",
    "        h, edge_index, edge_weights, batch, _, _ = self.pool2(h, edge_index, edge_weights, batch)\n",
    "\n",
    "        h = self.conv4(h, h, edge_index, edge_weights)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        h = self.global_pool(h, index=batch)\n",
    "        return h    \n",
    "    \n",
    "mol_batch.y[interacting_voxel_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTIVE CLASSIFIER\n",
    "#@title Classifier\n",
    "\n",
    "class ActiveClassifier(torch.nn.Module):\n",
    "    def __init__(self, pocket_model, poxel_model, molecule_model):\n",
    "        super(ActiveClassifier, self).__init__()\n",
    "        self.pocket_model = PocketGCN()\n",
    "        self.pox_pooler = PoxelPool()\n",
    "        self.mol_pooler = MoleculePool()\n",
    "\n",
    "#         self.linear1 = nn.Linear(2048, 1024)\n",
    "        self.linear1 = nn.Linear(1024, 512)\n",
    "        self.linear2 = nn.Linear(512, 512)\n",
    "        self.linear3 = nn.Linear(512, 1)\n",
    "\n",
    "#         self.linear1 = nn.Linear(512, 256)\n",
    "#         self.linear2 = nn.Linear(256, 64)\n",
    "#         self.linear3 = nn.Linear(64, 1)\n",
    "#         self.linear4 = nn.Linear(256, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, pocket_batch, active_batch, decoy_batch):\n",
    "#         voxel_node_indices = torch.where(pocket_batch.x[:, dummy_index] == 1.0)[0]\n",
    "#         non_voxel_node_indices = torch.where(pocket_batch.x[:, dummy_index] == 0)[0]\n",
    "#         voxel_edge_indices = torch.where(pocket_batch.edge_attr[:, voxel_edge_index]==1.0)[0]\n",
    "        \n",
    "        pocket_embeds = self.pocket_model(pocket_batch)\n",
    "        \n",
    "#         pocket_embeds[non_voxel_node_indices] = torch.zeros(pocket_embeds.size(1))\n",
    "#         trimmed_edge_index = torch.vstack((pocket_batch.edge_index[0][voxel_edge_indices],\n",
    "#                                            pocket_batch.edge_index[1][voxel_edge_indices]))\n",
    "        \n",
    "#         trimmed_edge_attr = pocket_batch.edge_attr[voxel_edge_indices]\n",
    "        \n",
    "        pocket_batch.x = pocket_embeds\n",
    "#         pocket_batch.edge_index = trimmed_edge_index\n",
    "#         pocket_batch.edge_attr = trimmed_edge_attr\n",
    "        \n",
    "        poxel_embeds = self.pox_pooler(pocket_batch)\n",
    "        active_embeds = self.mol_pooler(active_batch)\n",
    "        decoy_embeds = self.mol_pooler(decoy_batch)\n",
    "        \n",
    "        poxel_actives = torch.hstack((poxel_embeds, active_embeds))\n",
    "        poxel_decoys = torch.hstack((torch.cat([poxel_embeds]*len(decoy_embeds), dim=0), \n",
    "                                     decoy_embeds.repeat_interleave(poxel_embeds.size(0), dim=0)))\n",
    "        \n",
    "        all_embeds = torch.vstack((poxel_actives, poxel_decoys))\n",
    "\n",
    "        x = self.linear1(all_embeds) \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.linear2(x) \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.linear3(x) \n",
    "#         x = self.relu(x)\n",
    "        \n",
    "#         o = self.linear4(x) \n",
    "        return x\n",
    "    \n",
    "    \n",
    "# batch_size=32\n",
    "# ac = ActiveClassifier(PocketGCN, PoxelPool, MoleculePool)\n",
    "\n",
    "# corpus = pickle.load(open(corpus_dir + \"pdbbind_corpus_3_75.pkl\", 'rb'))\n",
    "# mol_loader = DataLoader(corpus[1], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# pocket_loader = DataLoader(corpus[2], batch_size=batch_size, shuffle=False)\n",
    "# decoy_batch = fetch_decoy_batch(200)\n",
    "\n",
    "# for active_batch, pocket_batch in zip(mol_loader, pocket_loader):\n",
    "#     out = ac(pocket_batch, active_batch, decoy_batch)\n",
    "#     print(out)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, target_dir in enumerate(root_dir):\n",
    "    \n",
    "    if idx < random.randint(0,100):\n",
    "        continue\n",
    "        \n",
    "    print(target_dir)\n",
    "    target_id = target_dir.split('/')[-2]\n",
    "    data = pickle.load(open(\"%s%s_ligand_graph.pkl\" % (target_dir, target_id), 'rb'))\n",
    "    print(data.edge_attr.shape)\n",
    "    break\n",
    "    \n",
    "#     g = torch_geometric.utils.to_networkx(data, to_undirected=True)\n",
    "#     nx.draw(g)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "sigmoid = nn.Sigmoid()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([100])).to(device)\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "ac = ActiveClassifier(PocketGCN, PoxelPool, MoleculePool).to(device)\n",
    "optimizer = torch.optim.Adam(ac.parameters(), lr=1e-3)\n",
    "\n",
    "def training_loop(epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"EPOCH %s\" % epoch)\n",
    "        \n",
    "        for corpus_idx in range(1,75):\n",
    "            loss_history = []\n",
    "            corpus = pickle.load(open(corpus_dir + \"pdbbind_corpus_%s_75.pkl\" % corpus_idx, 'rb'))\n",
    "            \n",
    "            mol_loader = DataLoader(corpus[1], batch_size=batch_size, shuffle=False)\n",
    "            pocket_loader = DataLoader(corpus[2], batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            for active_batch, pocket_batch in zip(mol_loader, pocket_loader):\n",
    "                optimizer.zero_grad() \n",
    "                active_batch = active_batch.to(device)\n",
    "                pocket_batch = pocket_batch.to(device)\n",
    "                decoy_batch = fetch_decoy_batch(batch_size).to(device)\n",
    "                \n",
    "                out = ac(pocket_batch, active_batch, decoy_batch)\n",
    "                \n",
    "                y = torch.zeros(out.size(0))\n",
    "                y[:batch_size] = 1\n",
    "                y = torch.unsqueeze(y, dim=1).to(device)\n",
    "                \n",
    "                loss = criterion(out, y)\n",
    "                loss_history.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            print(sum(loss_history) / len(loss_history))\n",
    "            print('active:', [float(\"%.3f\" % x) for x in torch.sigmoid(out[:7].squeeze()).tolist()])\n",
    "            print('decoy:', [float(\"%.3f\" % x) for x in torch.sigmoid(out[-7:].squeeze()).tolist()])\n",
    "            print('')\n",
    "    \n",
    "#       for batch_idx in range(int(POXEL_COLLECTION_SIZE / batch_size)):\n",
    "#         y = torch.hstack([torch.ones(batch_size), torch.zeros(batch_size*np_ratio)]).unsqueeze(dim=1).to(device) \n",
    "      \n",
    "#         pox_batch, active_batch, decoy_batch, active_indices = retrieve_training_batch(batch_size, np_ratio, poxel_collection, col_idx)\n",
    "    \n",
    "#         # pox_batch, active_batch, decoy_batch, active_indices = retrieve_DUMMY_batch(batch_size, np_ratio, poxel_collection, COL_IDX, [Ameans, Astds], [Bmeans, Bstds])\n",
    "#         out = ac(pox_batch.to(device), active_batch.to(device), decoy_batch.to(device), np_ratio)\n",
    "#         loss = criterion(out, y)\n",
    "#         print(batch_idx, torch.mean(sigmoid(out[0:10])).item(), torch.mean(sigmoid(out[10:])).item(), loss.item())\n",
    "\n",
    "        # print(loss)\n",
    "\n",
    "training_loop(100, 32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vs-env",
   "language": "python",
   "name": "vs-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
