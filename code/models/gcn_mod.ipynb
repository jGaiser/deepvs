{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import torch\n",
    "import torch_geometric\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GCN2Conv as GC2\n",
    "from torch_geometric.nn import GATv2Conv \n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, SiLU\n",
    "\n",
    "from math import log\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "\n",
    "# from ..inits import glorot\n",
    "from torch.nn.init import xavier_uniform_ as glorot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as config_file:  \n",
    "    config = yaml.safe_load(config_file) \n",
    "    \n",
    "with open(config['protein_config_file'], 'r') as config_file:  \n",
    "    protein_config = yaml.safe_load(config_file)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_corpus = pickle.load(open(config['interaction_voxel_graph_dir'] + \"interaction_voxel_corpus.pkl\", 'rb'))\n",
    "og_corpus = deepcopy(interaction_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = np.array(interaction_corpus[0])\n",
    "# graph_list = np.array(interaction_corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_EDGE_WEIGHT = 15.286330223083496\n",
    "\n",
    "interaction_corpus = deepcopy(og_corpus)\n",
    "\n",
    "for g_idx in range(len(interaction_corpus[1])):\n",
    "    updated_edge_index = deepcopy(interaction_corpus[1][g_idx].edge_index)\n",
    "    updated_edge_attr = deepcopy(interaction_corpus[1][g_idx].edge_attr)\n",
    "    updated_x = deepcopy(interaction_corpus[1][g_idx].x)\n",
    "    \n",
    "    for e_idx in range(updated_edge_attr.size(0)):\n",
    "        updated_edge_attr[e_idx][-1] /= MAX_EDGE_WEIGHT\n",
    "    \n",
    "    self_edges = torch.arange(updated_x.size(0)).unsqueeze(0)\n",
    "    self_edges = torch.cat((self_edges,self_edges),dim=0)\n",
    "    \n",
    "    updated_edge_index = torch.hstack((updated_edge_index, self_edges))\n",
    "    updated_edge_attr = torch.hstack((updated_edge_attr, torch.zeros((updated_edge_attr.size(0), 1))))\n",
    "    \n",
    "    self_loop_features = torch.zeros((updated_x.size(0), updated_edge_attr.size(1)))\n",
    "    self_loop_features[:,-1] = 1\n",
    "    \n",
    "    updated_edge_attr = torch.vstack((updated_edge_attr, self_loop_features))\n",
    "    x_coords = updated_x[:,-3:]\n",
    "    updated_x = updated_x[:,:-3]\n",
    "    \n",
    "    interaction_corpus[1][g_idx] = Data(x=updated_x, \n",
    "                                        x_coords=x_coords,\n",
    "                                        edge_index=updated_edge_index,\n",
    "                                        edge_attr=updated_edge_attr,\n",
    "                                        y=interaction_corpus[1][g_idx].y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for g_idx in range(len(interaction_corpus[1])):\n",
    "#     interaction_corpus[1][g_idx].edge_attr = torch.hstack((interaction_corpus[1][0].edge_attr,\n",
    "#                                                        torch.zeros((len(interaction_corpus[1][0].edge_attr)),1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(interaction_corpus[1], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = [0 for x in protein_config['interaction_labels']]\n",
    "\n",
    "for batch in loader:\n",
    "    for class_idx in batch.y:\n",
    "        class_count[class_idx] += 1\n",
    "        \n",
    "example_count = sum(class_count)\n",
    "class_max = max(class_count)\n",
    "class_weights = torch.tensor([1-x/example_count for x in class_count])\n",
    "# class_weights = torch.tensor([1/(x/class_max) for x in class_count])\n",
    "class_weights = torch.tensor([class_max/x for x in class_count])\n",
    "class_weights[4] = 60\n",
    "        \n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9216, 32])\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# pdist = torch.nn.PairwiseDistance(p=2) \n",
    "\n",
    "# class GC2Conv(MessagePassing):\n",
    "#     _cached_edge_index: Optional[Tuple[Tensor, Tensor]]\n",
    "#     _cached_adj_t: Optional[SparseTensor]\n",
    "\n",
    "#     def __init__(self, channels: int, alpha: float, theta: float = None,\n",
    "#                  layer: int = None, shared_weights: bool = True,\n",
    "#                  cached: bool = False, add_self_loops: bool = True,\n",
    "#                  normalize: bool = True, **kwargs):\n",
    "\n",
    "#         kwargs.setdefault('aggr', 'add')\n",
    "#         super().__init__(**kwargs)\n",
    "\n",
    "#         self.channels = channels\n",
    "#         self.alpha = alpha\n",
    "#         self.beta = 1.\n",
    "#         if theta is not None or layer is not None:\n",
    "#             assert theta is not None and layer is not None\n",
    "#             self.beta = log(theta / layer + 1)\n",
    "#         self.cached = cached\n",
    "#         self.normalize = normalize\n",
    "#         self.add_self_loops = add_self_loops\n",
    "\n",
    "#         self._cached_edge_index = None\n",
    "#         self._cached_adj_t = None\n",
    "\n",
    "#         self.weight1 = Parameter(torch.Tensor(channels, channels))\n",
    "\n",
    "#         if shared_weights:\n",
    "#             self.register_parameter('weight2', None)\n",
    "#         else:\n",
    "#             self.weight2 = Parameter(torch.Tensor(channels, channels))\n",
    "            \n",
    "#         MESSAGE_DIMS = channels*2+33\n",
    "#         self.message_conv = Seq(Linear(MESSAGE_DIMS, MESSAGE_DIMS,\n",
    "#                              SiLU(),\n",
    "#                              Linear(MESSAGE_DIMS, channels), )\n",
    "\n",
    "#         self.reset_parameters()\n",
    "        \n",
    "#     def reset_parameters(self):\n",
    "#         self.weight1 = Parameter(glorot(torch.Tensor(self.channels, self.channels)))\n",
    "#         self.weight2 = Parameter(glorot(torch.Tensor(self.channels, self.channels)))\n",
    "        \n",
    "#         self._cached_edge_index = None\n",
    "#         self._cached_adj_t = None\n",
    "\n",
    "#     def forward(self, x: Tensor, x_0: Tensor, edge_index: Adj,\n",
    "#                 edge_weight: OptTensor=None, x_coords=None, edge_features=None) -> Tensor:\n",
    "\n",
    "#         if self.normalize:\n",
    "#             if isinstance(edge_index, Tensor):\n",
    "#                 cache = self._cached_edge_index\n",
    "#                 if cache is None:\n",
    "#                     edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
    "#                         edge_index, edge_weight, x.size(self.node_dim), False,\n",
    "#                         self.add_self_loops, self.flow, dtype=x.dtype)\n",
    "#                     if self.cached:\n",
    "#                         self._cached_edge_index = (edge_index, edge_weight)\n",
    "#                 else:\n",
    "#                     edge_index, edge_weight = cache[0], cache[1]\n",
    "\n",
    "#             elif isinstance(edge_index, SparseTensor):\n",
    "#                 cache = self._cached_adj_t\n",
    "#                 if cache is None:\n",
    "#                     edge_index = gcn_norm(  # yapf: disable\n",
    "#                         edge_index, edge_weight, x.size(self.node_dim), False,\n",
    "#                         self.add_self_loops, self.flow, dtype=x.dtype)\n",
    "#                     if self.cached:\n",
    "#                         self._cached_adj_t = edge_index\n",
    "#                 else:\n",
    "#                     edge_index = cache\n",
    "\n",
    "#         # propagate_type: (x: Tensor, edge_weight: OptTensor)\n",
    "#         x = self.propagate(edge_index, x_coords=x_coords, x=x, edge_weight=edge_weight, edge_features=edge_features, size=None)\n",
    "\n",
    "#         x.mul_(1 - self.alpha)\n",
    "#         x_0 = self.alpha * x_0[:x.size(0)]\n",
    "\n",
    "#         if self.weight2 is None:\n",
    "#             out = x.add_(x_0)\n",
    "#             out = torch.addmm(out, out, self.weight1, beta=1. - self.beta,\n",
    "#                               alpha=self.beta)\n",
    "#         else:\n",
    "#             out = torch.addmm(x, x, self.weight1, beta=1. - self.beta,\n",
    "#                               alpha=self.beta)\n",
    "#             out = out + torch.addmm(x_0, x_0, self.weight2,\n",
    "#                                     beta=1. - self.beta, alpha=self.beta)\n",
    "\n",
    "#         return out\n",
    "\n",
    "\n",
    "#     def message(self, x_i: Tensor, x_j: Tensor, x_coords_i: Tensor, x_coords_j: Tensor, edge_weight: OptTensor, edge_features: Tensor) -> Tensor:\n",
    "#         node_dist = pdist(x_coords_i, x_coords_j) / MAX_EDGE_WEIGHT\n",
    "         \n",
    "#         return x_j if edge_weight is None else node_dist.view(-1, 1) * x_j\n",
    "\n",
    "#     def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "#         return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         return (f'{self.__class__.__name__}({self.channels}, '\n",
    "#                 f'alpha={self.alpha}, beta={self.beta})')\n",
    "    \n",
    "    \n",
    "# hidden = 512 \n",
    "# INTERACTION_TYPES = protein_config['interaction_labels']\n",
    "# NODE_DIMS = 38 \n",
    "# EDGE_DIMS = 9\n",
    "# DUMMY_INDEX = protein_config['atom_labels'].index('DUMMY')\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.linear1 = torch.nn.Linear(NODE_DIMS,hidden)\n",
    "#         self.lin_edge = torch.nn.Linear(EDGE_DIMS-1, 32)\n",
    "        \n",
    "#         self.conv1 = GC2Conv(hidden, 0.2, add_self_loops=False)\n",
    "#         self.conv2 = GC2Conv(hidden, 0.2, add_self_loops=False)\n",
    "#         self.conv3 = GC2Conv(hidden, 0.2, add_self_loops=False)\n",
    "\n",
    "#         self.linear2 = torch.nn.Linear(hidden, len(INTERACTION_TYPES))\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         x, x_coords, edge_index, edge_features = data.x, data.x_coords, data.edge_index, torch.hstack((data.edge_attr[:,:-2], data.edge_attr[:,-1:]))\n",
    "\n",
    "#         edge_features = self.lin_edge(edge_features)\n",
    "#         x = self.linear1(x)\n",
    "\n",
    "#         h = self.conv1(x, x, edge_index, x_coords=x_coords, edge_features=edge_features)\n",
    "#         h = F.relu(h)\n",
    "\n",
    "#         h = self.conv2(h, x, edge_index, x_coords=x_coords, edge_features=edge_features)\n",
    "#         h = F.relu(h)\n",
    "\n",
    "# #         h = self.conv2(h, x, edge_index, edge_weights)\n",
    "# #         h = F.relu(h)\n",
    "        \n",
    "#         o = self.linear2(h)\n",
    "#         return o\n",
    "\n",
    "# model = GCN()\n",
    "\n",
    "# for batch in loader:\n",
    "#     print(model(batch))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist = torch.nn.PairwiseDistance(p=2) \n",
    "\n",
    "class GC2Conv(MessagePassing):\n",
    "    _cached_edge_index: Optional[Tuple[Tensor, Tensor]]\n",
    "    _cached_adj_t: Optional[SparseTensor]\n",
    "\n",
    "    def __init__(self, channels: int, alpha: float, theta: float = None,\n",
    "                 layer: int = None, shared_weights: bool = True,\n",
    "                 cached: bool = False, add_self_loops: bool = True,\n",
    "                 normalize: bool = True, **kwargs):\n",
    "\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.channels = channels\n",
    "        self.alpha = alpha\n",
    "        self.beta = 1.\n",
    "        if theta is not None or layer is not None:\n",
    "            assert theta is not None and layer is not None\n",
    "            self.beta = log(theta / layer + 1)\n",
    "        self.cached = cached\n",
    "        self.normalize = normalize\n",
    "        self.add_self_loops = add_self_loops\n",
    "\n",
    "        self._cached_edge_index = None\n",
    "        self._cached_adj_t = None\n",
    "\n",
    "        self.weight1 = Parameter(torch.Tensor(channels, channels))\n",
    "\n",
    "        if shared_weights:\n",
    "            self.register_parameter('weight2', None)\n",
    "        else:\n",
    "            self.weight2 = Parameter(torch.Tensor(channels, channels))\n",
    "            \n",
    "        MESSAGE_DIMS = channels*2+33\n",
    "        self.message_conv = Seq(Linear(MESSAGE_DIMS, MESSAGE_DIMS,\n",
    "                             SiLU(),\n",
    "                             Linear(MESSAGE_DIMS, channels), )\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.weight1 = Parameter(glorot(torch.Tensor(self.channels, self.channels)))\n",
    "        self.weight2 = Parameter(glorot(torch.Tensor(self.channels, self.channels)))\n",
    "        \n",
    "        self._cached_edge_index = None\n",
    "        self._cached_adj_t = None\n",
    "\n",
    "    def forward(self, x: Tensor, x_0: Tensor, edge_index: Adj,\n",
    "                edge_weight: OptTensor=None, x_coords=None, edge_features=None) -> Tensor:\n",
    "\n",
    "        if self.normalize:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                cache = self._cached_edge_index\n",
    "                if cache is None:\n",
    "                    edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
    "                        edge_index, edge_weight, x.size(self.node_dim), False,\n",
    "                        self.add_self_loops, self.flow, dtype=x.dtype)\n",
    "                    if self.cached:\n",
    "                        self._cached_edge_index = (edge_index, edge_weight)\n",
    "                else:\n",
    "                    edge_index, edge_weight = cache[0], cache[1]\n",
    "\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                cache = self._cached_adj_t\n",
    "                if cache is None:\n",
    "                    edge_index = gcn_norm(  # yapf: disable\n",
    "                        edge_index, edge_weight, x.size(self.node_dim), False,\n",
    "                        self.add_self_loops, self.flow, dtype=x.dtype)\n",
    "                    if self.cached:\n",
    "                        self._cached_adj_t = edge_index\n",
    "                else:\n",
    "                    edge_index = cache\n",
    "\n",
    "        # propagate_type: (x: Tensor, edge_weight: OptTensor)\n",
    "        x = self.propagate(edge_index, x_coords=x_coords, x=x, edge_weight=edge_weight, edge_features=edge_features, size=None)\n",
    "\n",
    "        x.mul_(1 - self.alpha)\n",
    "        x_0 = self.alpha * x_0[:x.size(0)]\n",
    "\n",
    "        if self.weight2 is None:\n",
    "            out = x.add_(x_0)\n",
    "            out = torch.addmm(out, out, self.weight1, beta=1. - self.beta,\n",
    "                              alpha=self.beta)\n",
    "        else:\n",
    "            out = torch.addmm(x, x, self.weight1, beta=1. - self.beta,\n",
    "                              alpha=self.beta)\n",
    "            out = out + torch.addmm(x_0, x_0, self.weight2,\n",
    "                                    beta=1. - self.beta, alpha=self.beta)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_i: Tensor, x_j: Tensor, x_coords_i: Tensor, x_coords_j: Tensor, edge_weight: OptTensor, edge_features: Tensor) -> Tensor:\n",
    "        node_dist = pdist(x_coords_i, x_coords_j) / MAX_EDGE_WEIGHT\n",
    "         \n",
    "        return x_j if edge_weight is None else node_dist.view(-1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.channels}, '\n",
    "                f'alpha={self.alpha}, beta={self.beta})')\n",
    "    \n",
    "    \n",
    "hidden = 512 \n",
    "INTERACTION_TYPES = protein_config['interaction_labels']\n",
    "NODE_DIMS = 38 \n",
    "EDGE_DIMS = 9\n",
    "DUMMY_INDEX = protein_config['atom_labels'].index('DUMMY')\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(NODE_DIMS,hidden)\n",
    "        self.lin_edge = torch.nn.Linear(EDGE_DIMS-1, 32)\n",
    "        \n",
    "        self.conv1 = GC2Conv(hidden, 0.2, add_self_loops=False)\n",
    "        self.conv2 = GC2Conv(hidden, 0.2, add_self_loops=False)\n",
    "        self.conv3 = GC2Conv(hidden, 0.2, add_self_loops=False)\n",
    "\n",
    "        self.linear2 = torch.nn.Linear(hidden, len(INTERACTION_TYPES))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, x_coords, edge_index, edge_features = data.x, data.x_coords, data.edge_index, torch.hstack((data.edge_attr[:,:-2], data.edge_attr[:,-1:]))\n",
    "\n",
    "        edge_features = self.lin_edge(edge_features)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        h = self.conv1(x, x, edge_index, x_coords=x_coords, edge_features=edge_features)\n",
    "        h = F.relu(h)\n",
    "\n",
    "        h = self.conv2(h, x, edge_index, x_coords=x_coords, edge_features=edge_features)\n",
    "        h = F.relu(h)\n",
    "\n",
    "#         h = self.conv2(h, x, edge_index, edge_weights)\n",
    "#         h = F.relu(h)\n",
    "        \n",
    "        o = self.linear2(h)\n",
    "        return o\n",
    "\n",
    "model = GCN()\n",
    "\n",
    "for batch in loader:\n",
    "    print(model(batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_function = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "batch_idx = 0\n",
    "for epoch in range(5000):\n",
    "    avg_loss = []\n",
    "    print(\"EPOCH %s\" % epoch)\n",
    "    \n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        dummy_indices = torch.where(batch.x[:,DUMMY_INDEX] == 1)\n",
    "        \n",
    "        out = model(batch)\n",
    "        \n",
    "        dummy_node_out = out[dummy_indices]\n",
    "        loss = loss_function(dummy_node_out, batch.y)\n",
    "        avg_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_idx += 1\n",
    "    \n",
    "    print(\"Average loss:\", sum(avg_loss) / len(avg_loss))\n",
    "    print(torch.argmax(dummy_node_out,dim=1)[:15])\n",
    "    print(batch.y[:15])\n",
    "    avg_loss = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepvs-env",
   "language": "python",
   "name": "deepvs-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
